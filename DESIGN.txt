Design Document
This project initially had three possible goals:
	-A good outcome: Creating a website which will take in text, find misinformation based on data from Politifact and highlight it.
	-A better outcome: Creating a chrome extension to take in text, then processing it the same way.
	-A best outcome to find similar statements to misinformation, not just identical ones
At the end, we managed to complete the good and better outcome.

When first creating the program, we had 3 different problems to deal with. 
	First was obtaining the necessary data from Politifact. This was done using a Python webscraper, which would process each webpage in Politifact's most recent articles pages. This would be converted into a .csv format to be processed by our server.
	
	Second was creating the server. The server would have to store each misinformation statement in an easily searchable way. For this we used a HashMap. In order to speed up the server, we used multithreading, and in order to deal with concurrency issues, we used a ConcurrentHashMap instead (from Kevin's suggestion). We also used the Sun Http package in order to easily send information over the internet.
	
	Third was creating the client. The client was an HTML file on the server. This client had to take in text using a form, split the text up properly, send this text to the server, and produce the edited text based on the server's response. The text was split into sentences (on the client and server side) using RegEx. The text was sent to the server using the URL, which would be in this format: [server].com/query?s=[URI-encoded-text]. The text was then send using synchronous xhttp requests. The server would send the relevant data (the url and type of misinformation) in JSON, which would be parsed and used.
	
The chrome extension was definitely quite challenging, and as a result, while the extension is working, it is not ready to be used normally, due to severe memory performance issues. The first new challenge of the chrome extension was being able to take in all the text from a page. While this might seem easy at first, the issue is that modern websites often use a technique called AJAX in order to send and receive data from servers which are not necessarily in the initial HTML. In fact, we tried to deal with this issue using regular JavaScript, but in the end decided to re-do all the code using jQuery in order to handle this issue easily. The second problem was not causing the broswer to hang. While initially we used synchronous xhttp requests to get data from the server, when done with a chrome extension, this would cause the website to hang. Since our server is running on just one dyno on Heroku, it can take a long time for each request to be handled. This would not work from a usability perspective. No client is going to wait 10-15 seconds just for the loaded page to be processed. Therefore, we decided to use AJAX asynchronously. This meant that the program would allow the website to be responsive while processing the text. The server could also deal with multiple requests due to its multithreading. However, we did run into issues with memory, as stated. These issues shouldn't be too difficult to deal with, but would require significant changes to the code (we believe). There were also smaller issues to be dealt with (especially because none of us had any experience in JavaScript or jQuery until now), but these were relatively simple and are addressed in the comments.